{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Neural Network\n",
    "\n",
    "This is inspired by <https://pub.towardsai.net/building-neural-networks-from-scratch-with-python-code-and-math-in-detail-i-536fae5d7bbf>'s \"case study\" with corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import io\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "import matplotlib\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv = \"\"\"\n",
    "observation,input1,input2,output\n",
    "1,0,0,0\n",
    "2,0,1,1\n",
    "3,1,0,1\n",
    "4,1,1,1\n",
    "\"\"\"\n",
    "\n",
    "dataset = pandas.read_csv(io.StringIO(input_csv), index_col=\"observation\")\n",
    "inputs = dataset.iloc[:,:-1].to_numpy().astype('float32')\n",
    "ground_truth = dataset.iloc[:,-1].to_numpy().reshape(-1, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "[[0. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]]\n",
      "Truth: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Inputs:\\n{}\".format(inputs))\n",
    "print(\"Truth: \\n{}\".format(ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.05\n",
    "NUM_ITERATIONS = 10000\n",
    "NUM_NEURONS_HIDDEN = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By Hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, weights, bias):\n",
    "    return numpy.dot(x, weights) + bias\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + numpy.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    y = sigmoid(x)\n",
    "    return y * (1.0 - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hidden layer weights:\n",
      "[[4.17022005e-01 7.20324493e-01 1.14374817e-04]\n",
      " [3.02332573e-01 1.46755891e-01 9.23385948e-02]]\n",
      "\n",
      "Starting output layer weights:\n",
      "[[0.18626021]\n",
      " [0.34556073]\n",
      " [0.39676747]]\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(seed=1)\n",
    "weights_hidden = numpy.random.rand(inputs.shape[1], NUM_NEURONS_HIDDEN)\n",
    "weights_output = numpy.random.rand(NUM_NEURONS_HIDDEN, 1)\n",
    "# bias = numpy.random.rand(1)[0]\n",
    "bias = 0.0\n",
    "\n",
    "print(\"Starting hidden layer weights:\\n{}\\n\".format(weights_hidden))\n",
    "print(\"Starting output layer weights:\\n{}\".format(weights_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our perceptron, \n",
    "\n",
    "$ y(x) = x_1 * w_1 + x_2 * w_2 + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the network\n",
    "\n",
    "We use a neural network with the same number of inputs and outputs before (two inputs ($x_1$ and $x_2$), one output ($f$), but this time we add a _hidden layer_ in between with three weights for each input:\n",
    "\n",
    "[![](https://mermaid.ink/img/eyJjb2RlIjoiZ3JhcGggTFJcbiAgICB4MS0tIHYxIC0tLSBnMShnKVxuICAgIHgxLS0gdjEgLS0tIGcyKGcpXG4gICAgeDEtLSB2MSAtLS0gZzMoZylcbiAgICB4Mi0tIHYyIC0tLSBnMShnKVxuICAgIHgyLS0gdjIgLS0tIGcyKGcpXG4gICAgeDItLSB2MiAtLS0gZzMoZylcbiAgICBnMShnKS0tIHcxIC0tLSBmXG4gICAgZzIoZyktLSB3MiAtLS0gZlxuICAgIGczKGcpLS0gdzMgLS0tIGYiLCJtZXJtYWlkIjp7InRoZW1lIjoiZGVmYXVsdCIsImZsb3djaGFydCI6eyJjdXJ2ZSI6ImJhc2lzIn19LCJ1cGRhdGVFZGl0b3IiOmZhbHNlfQ)](https://mermaid-js.github.io/mermaid-live-editor/#/edit/eyJjb2RlIjoiZ3JhcGggTFJcbiAgICB4MS0tIHYxIC0tLSBnMShnKVxuICAgIHgxLS0gdjEgLS0tIGcyKGcpXG4gICAgeDEtLSB2MSAtLS0gZzMoZylcbiAgICB4Mi0tIHYyIC0tLSBnMShnKVxuICAgIHgyLS0gdjIgLS0tIGcyKGcpXG4gICAgeDItLSB2MiAtLS0gZzMoZylcbiAgICBnMShnKS0tIHcxIC0tLSBmXG4gICAgZzIoZyktLSB3MiAtLS0gZlxuICAgIGczKGcpLS0gdzMgLS0tIGYiLCJtZXJtYWlkIjp7InRoZW1lIjoiZGVmYXVsdCIsImZsb3djaGFydCI6eyJjdXJ2ZSI6ImJhc2lzIn19LCJ1cGRhdGVFZGl0b3IiOmZhbHNlfQ)\n",
    "\n",
    "These network diagrams are confusing because the definition of each node is not consistency.  So let's define our layers:\n",
    "\n",
    "1. input values: $x$\n",
    "2. hidden layer: $\\hat{y} = \\mathbf{x} \\cdot \\mathbf{v} $\n",
    "  - $\\hat{y}_{11} = x_{11} v_{11} + x_{12} v_{21} $\n",
    "  - $\\hat{y}_{21} = x_{21} v_{11} + x_{22} v_{21} $\n",
    "  - ...\n",
    "  - $\\hat{y}_{42} = x_{41} v_{12} + x_{42} v_{22} $\n",
    "  - $\\hat{y}_{43} = x_{41} v_{13} + x_{42} v_{23} $\n",
    "3. activation of hidden layer: $ g(\\hat{y}) = \\frac{1}{1+e^{-\\hat{y}}} $\n",
    "4. output layer: $y = \\mathbf{g} \\cdot \\mathbf{w}$\n",
    "  - $y_{11} = g_{11} w_{11} + g_{12} w_{21} + g_{13} w_{31} $\n",
    "  - ...\n",
    "  - $y_{41} = g_{41} w_{11} + g_{42} w_{21} + g_{43} w_{31} $\n",
    "5. activation of output layer: $ f(y) = \\frac{1}{1+e^{-y}} $\n",
    "\n",
    "So our neural network with its hidden layer is really\n",
    "\n",
    "$ f(y(g(\\hat{y}(x)))) $\n",
    "\n",
    "We choose mean-squared error as our loss function:\n",
    "\n",
    "$ E = (f - f_0)^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output layer (\"phase 1\")\n",
    "\n",
    "$ \\frac{\\partial E}{\\partial w} =\n",
    "\\frac{\\partial E}{\\partial f} \\cdot\n",
    "\\frac{\\partial f}{\\partial y} \\cdot\n",
    "\\frac{\\partial y}{\\partial w}\n",
    "$\n",
    "\n",
    "where\n",
    "\n",
    "- $ \\frac{\\partial E}{\\partial f} = 2 ( f - f_0 ) $\n",
    "- $ \\frac{\\partial f}{\\partial y} = f (1 - f) $\n",
    "- $ \\frac{\\partial y}{\\partial w} = g$ (in the single-layer case, $ \\frac{\\partial y}{\\partial w} = x $)\n",
    "\n",
    "#### Hidden layer (\"phase 2\")\n",
    "\n",
    "$ \\frac{\\partial E}{\\partial v} = \n",
    "\\frac{\\partial E}{\\partial g}\n",
    "\\frac{\\partial g}{\\partial \\hat{y}}\n",
    "\\frac{\\partial \\hat{y}}{\\partial v} $\n",
    "\n",
    "where\n",
    "\n",
    "- $ \\frac{\\partial E}{\\partial g} =\n",
    "           \\frac{\\partial E}{\\partial f} \\cdot\n",
    "           \\frac{\\partial f}{\\partial y} \\cdot\n",
    "           \\frac{\\partial y}{\\partial g} $\n",
    "    - $ \\frac{\\partial E}{\\partial g} =\n",
    "        \\frac{\\partial E}{\\partial y} \\cdot\n",
    "        \\frac{\\partial y}{\\partial g} $\n",
    "    - $ \\frac{\\partial E}{\\partial y} =\n",
    "        \\frac{\\partial E}{\\partial f} \\cdot\n",
    "        \\frac{\\partial f}{\\partial y}$\n",
    "- $ \\frac{\\partial g}{\\partial \\hat{y}} = g (1 - g)$\n",
    "- $ \\frac{\\partial \\hat{y}}{\\partial v} = x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error at step     0: 1.960156e-01\n",
      "error at step  1000: 1.651662e-01\n",
      "error at step  2000: 1.593565e-01\n",
      "error at step  3000: 1.550351e-01\n",
      "error at step  4000: 1.511067e-01\n",
      "error at step  5000: 1.465351e-01\n",
      "error at step  6000: 1.392272e-01\n",
      "error at step  7000: 1.234844e-01\n",
      "error at step  8000: 9.350530e-02\n",
      "error at step  9000: 6.205274e-02\n",
      "Final weights:\n",
      "       hidden: [ 1.66707072  2.43454525 -1.50667653  1.66659174  2.29166158 -1.50681196]\n",
      "       output: [ 0.70570663  2.04736287 -4.12867523]\n",
      "Final bias:    0.0\n",
      "10000 iterations took 2.1 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "for i in range(NUM_ITERATIONS):\n",
    "    w = weights_output\n",
    "    v = weights_hidden\n",
    "    yhat = linear(inputs, v, bias)\n",
    "    g = sigmoid(yhat)\n",
    "    y = linear(g, w, bias)\n",
    "    f = sigmoid(y)\n",
    "    \n",
    "    error = numpy.power(f - ground_truth, 2)\n",
    "    if i % (NUM_ITERATIONS / 10) == 0:\n",
    "        print(\"error at step {:5d}: {:10.6e}\".format(i, error.mean()))\n",
    "\n",
    "    # from the tutorial\n",
    "    # dE_df = f - f0\n",
    "    # df_dy = f * (1 - f)\n",
    "    # dy_dw = g\n",
    "    # dE_dw = numpy.dot(dy_dw.T, dE_df * df_dy)\n",
    "    \n",
    "    # calculate out partial derivatives for each input\n",
    "    dE_df = 0.5 * (f - ground_truth)\n",
    "    df_dy = f * (1.0 - f)\n",
    "    dy_dw = g\n",
    "    dE_dy = dE_df * df_dy\n",
    "    dE_dw = numpy.dot(dy_dw.T, dE_dy)\n",
    "    \n",
    "    # from the tutorial\n",
    "#   dE_dy = dE_df * df_dy\n",
    "#   dy_dg = w\n",
    "#   dg_dyhat = g * (1.0 - g)\n",
    "#   dE_dg = numpy.dot(dE_dy, dy_dg.T)\n",
    "#   dyhat_dv = inputs\n",
    "#   dE_dv = numpy.dot(dyhat_dv.T, dg_dyhat * dE_dg)\n",
    "    \n",
    "    # calculate partial derivatives with respect to hidden layer weights\n",
    "    dy_dg = w\n",
    "    dE_dg = numpy.dot(dE_dy, dy_dg.T)\n",
    "    dg_dyhat = g * (1.0 - g)\n",
    "    dyhat_dv = inputs\n",
    "    dE_dyhat = dE_dg * dg_dyhat\n",
    "    dE_dv = numpy.dot(dyhat_dv.T, dE_dyhat)\n",
    "    \n",
    "    # update weights and biases - the error is the sum of error over each input\n",
    "    if NUM_ITERATIONS == 1:\n",
    "        print(\"dE_dv:\\n{}\".format(dE_dv))\n",
    "        print(\"dE_dw:\\n{}\".format(dE_dw))\n",
    "    weights_hidden -= LEARNING_RATE * dE_dv\n",
    "    weights_output -= LEARNING_RATE * dE_dw\n",
    "#   bias -= LEARNING_RATE * dE_dy.sum()\n",
    "\n",
    "print(\"Final weights:\")\n",
    "print(\"       hidden: {}\".format(weights_hidden.flatten()))\n",
    "print(\"       output: {}\".format(weights_output.flatten()))\n",
    "print(\"Final bias:    {}\".format(bias))\n",
    "print(\"{:d} iterations took {:.1f} seconds\".format(NUM_ITERATIONS, time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input1</th>\n",
       "      <th>input2</th>\n",
       "      <th>output</th>\n",
       "      <th>prediction</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>observation</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.334522</td>\n",
       "      <td>-0.334522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.846053</td>\n",
       "      <td>0.153947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.849022</td>\n",
       "      <td>0.150978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.925358</td>\n",
       "      <td>0.074642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             input1  input2  output  prediction     error\n",
       "observation                                              \n",
       "1                 0       0       0    0.334522 -0.334522\n",
       "2                 0       1       1    0.846053  0.153947\n",
       "3                 1       0       1    0.849022  0.150978\n",
       "4                 1       1       1    0.925358  0.074642"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# traverse the hidden layer\n",
    "predicted_output = linear(inputs, weights_hidden, bias)\n",
    "predicted_output = sigmoid(predicted_output)\n",
    "# traverse the output layer\n",
    "predicted_output = linear(predicted_output, weights_output, bias)\n",
    "predicted_output = sigmoid(predicted_output)\n",
    "\n",
    "# convert output to dataframe\n",
    "predicted_output = pandas.DataFrame(\n",
    "    predicted_output,\n",
    "    columns=[\"prediction\"],\n",
    "    index=dataset.index)\n",
    "\n",
    "output = pandas.concat(\n",
    "    (dataset, predicted_output),\n",
    "    axis=1)\n",
    "output['error'] = output['output'] - output['prediction']\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(0)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(inputs.shape[1], NUM_NEURONS_HIDDEN, bias=False),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(NUM_NEURONS_HIDDEN, 1, bias=False),\n",
    "    torch.nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hidden layer weights:\n",
      "Parameter containing:\n",
      "tensor([[4.1702e-01, 3.0233e-01],\n",
      "        [7.2032e-01, 1.4676e-01],\n",
      "        [1.1437e-04, 9.2339e-02]], dtype=torch.float64, requires_grad=True)\n",
      "\n",
      "Starting output layer weights:\n",
      "Parameter containing:\n",
      "tensor([[0.1863, 0.3456, 0.3968]], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(seed=1) # use same initial seed as before\n",
    "\n",
    "with torch.no_grad():\n",
    "    # torch.rand() is faster, but we use numpy to re-use the same random starting weights/biases\n",
    "    model[0].weight = torch.nn.Parameter(torch.from_numpy(numpy.random.rand(inputs.shape[1], NUM_NEURONS_HIDDEN).T))\n",
    "    # model[0].bias = torch.nn.Parameter(torch.from_numpy(numpy.random.rand(1, 1)))\n",
    "    model[2].weight = torch.nn.Parameter(torch.from_numpy(numpy.random.rand(NUM_NEURONS_HIDDEN, 1).T))\n",
    "print(\"Starting hidden layer weights:\\n{}\\n\".format(model[0].weight))\n",
    "print(\"Starting output layer weights:\\n{}\".format(model[2].weight))\n",
    "# print(\"Starting bias: {}\".format(model[0].bias.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error at step     0: 1.960156e-01\n",
      "error at step  1000: 1.651662e-01\n",
      "error at step  2000: 1.593565e-01\n",
      "error at step  3000: 1.550351e-01\n",
      "error at step  4000: 1.511067e-01\n",
      "error at step  5000: 1.465351e-01\n",
      "error at step  6000: 1.392272e-01\n",
      "error at step  7000: 1.234844e-01\n",
      "error at step  8000: 9.350530e-02\n",
      "error at step  9000: 6.205274e-02\n",
      "Final weights:\n",
      "       hidden: [ 1.66707072  1.66659174  2.43454525  2.29166158 -1.50667653 -1.50681196]\n",
      "       output: [ 0.70570663  2.04736287 -4.12867523]\n",
      "10000 iterations took 10.3 seconds\n"
     ]
    }
   ],
   "source": [
    "inputs_tensor = torch.from_numpy(inputs.astype(numpy.float64))\n",
    "truth_tensor = torch.from_numpy(ground_truth.reshape(-1, 1).astype(numpy.float64))\n",
    "\n",
    "loss = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "model.train()\n",
    "t0 = time.time()\n",
    "for i in range(NUM_ITERATIONS):\n",
    "    f = model(inputs_tensor)\n",
    "\n",
    "    error = loss(f, truth_tensor)\n",
    "    if i % (NUM_ITERATIONS / 10) == 0:\n",
    "        print(\"error at step {:5d}: {:10.6e}\".format(i, error.mean()))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    error.backward()\n",
    "\n",
    "    if NUM_ITERATIONS == 1:\n",
    "        print(\"dE_dv:\\n{}\".format(model[0].weight.grad.detach().numpy()))\n",
    "        print(\"dE_dw:\\n{}\".format(model[2].weight.grad.detach().numpy()))\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "print(\"Final weights:\")\n",
    "print(\"       hidden: {}\".format(next(model[0].parameters()).detach().numpy().flatten()))\n",
    "print(\"       output: {}\".format(next(model[2].parameters()).detach().numpy().flatten()))\n",
    "\n",
    "print(\"{:d} iterations took {:.1f} seconds\".format(NUM_ITERATIONS, time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input1</th>\n",
       "      <th>input2</th>\n",
       "      <th>output</th>\n",
       "      <th>prediction</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>observation</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.334522</td>\n",
       "      <td>-0.334522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.846053</td>\n",
       "      <td>0.153947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.849022</td>\n",
       "      <td>0.150978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.925358</td>\n",
       "      <td>0.074642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             input1  input2  output  prediction     error\n",
       "observation                                              \n",
       "1                 0       0       0    0.334522 -0.334522\n",
       "2                 0       1       1    0.846053  0.153947\n",
       "3                 1       0       1    0.849022  0.150978\n",
       "4                 1       1       1    0.925358  0.074642"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "predicted_output = model(inputs_tensor).detach().numpy()\n",
    "predicted_output = pandas.DataFrame(\n",
    "    predicted_output,\n",
    "    columns=[\"prediction\"],\n",
    "    index=dataset.index)\n",
    "\n",
    "output = pandas.concat(\n",
    "    (dataset, predicted_output),\n",
    "    axis=1)\n",
    "output['error'] = output['output'] - output['prediction']\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
