{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Multilayer Neural Network\n",
    "\n",
    "This is inspired by <https://pub.towardsai.net/building-neural-networks-with-python-code-and-math-in-detail-ii-bbe8accbf3d1> with corrections to the loss function to match what PyTorch does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import io\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv = \"\"\"\n",
    "observation,input1,input2,output\n",
    "1,0,0,0\n",
    "2,0,1,1\n",
    "3,1,0,1\n",
    "4,1,1,1\n",
    "\"\"\"\n",
    "\n",
    "dataset = pandas.read_csv(io.StringIO(input_csv), index_col=\"observation\")\n",
    "inputs = dataset.iloc[:,:-1].to_numpy().astype('float32')\n",
    "ground_truth = dataset.iloc[:,-1].to_numpy().reshape(-1, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "[[0. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]]\n",
      "Truth: \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Inputs:\\n{}\".format(inputs))\n",
    "print(\"Truth: \\n{}\".format(ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.05\n",
    "NUM_ITERATIONS = 10000\n",
    "NUM_NEURONS_HIDDEN = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By Hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, weights, bias):\n",
    "    return numpy.dot(x, weights) + bias\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + numpy.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hidden layer weights:\n",
      "[[4.17022005e-01 7.20324493e-01 1.14374817e-04]\n",
      " [3.02332573e-01 1.46755891e-01 9.23385948e-02]]\n",
      "\n",
      "Starting output layer weights:\n",
      "[[0.18626021]\n",
      " [0.34556073]\n",
      " [0.39676747]]\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(seed=1)\n",
    "weights_hidden = numpy.random.rand(inputs.shape[1], NUM_NEURONS_HIDDEN)\n",
    "weights_output = numpy.random.rand(NUM_NEURONS_HIDDEN, 1)\n",
    "# bias = numpy.random.rand(1)[0]\n",
    "bias = 0.0\n",
    "\n",
    "print(\"Starting hidden layer weights:\\n{}\\n\".format(weights_hidden))\n",
    "print(\"Starting output layer weights:\\n{}\".format(weights_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our perceptron, \n",
    "\n",
    "$ y(x) = x_1 * w_1 + x_2 * w_2 + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the network\n",
    "\n",
    "We use a neural network with the same number of inputs and outputs before--two inputs ($x_1$ and $x_2$), one output ($f$).  However, this time we add a _hidden layer_ in between with three weights for each input:\n",
    "\n",
    "[![](https://mermaid.ink/img/eyJjb2RlIjoiZ3JhcGggTFJcbiAgICB4MS0tIHYxIC0tLSBnMShnKVxuICAgIHgxLS0gdjEgLS0tIGcyKGcpXG4gICAgeDEtLSB2MSAtLS0gZzMoZylcbiAgICB4Mi0tIHYyIC0tLSBnMShnKVxuICAgIHgyLS0gdjIgLS0tIGcyKGcpXG4gICAgeDItLSB2MiAtLS0gZzMoZylcbiAgICBnMShnKS0tIHcxIC0tLSBmXG4gICAgZzIoZyktLSB3MiAtLS0gZlxuICAgIGczKGcpLS0gdzMgLS0tIGYiLCJtZXJtYWlkIjp7InRoZW1lIjoiZGVmYXVsdCIsImZsb3djaGFydCI6eyJjdXJ2ZSI6ImJhc2lzIn19LCJ1cGRhdGVFZGl0b3IiOmZhbHNlfQ)](https://mermaid-js.github.io/mermaid-live-editor/#/edit/eyJjb2RlIjoiZ3JhcGggTFJcbiAgICB4MS0tIHYxIC0tLSBnMShnKVxuICAgIHgxLS0gdjEgLS0tIGcyKGcpXG4gICAgeDEtLSB2MSAtLS0gZzMoZylcbiAgICB4Mi0tIHYyIC0tLSBnMShnKVxuICAgIHgyLS0gdjIgLS0tIGcyKGcpXG4gICAgeDItLSB2MiAtLS0gZzMoZylcbiAgICBnMShnKS0tIHcxIC0tLSBmXG4gICAgZzIoZyktLSB3MiAtLS0gZlxuICAgIGczKGcpLS0gdzMgLS0tIGYiLCJtZXJtYWlkIjp7InRoZW1lIjoiZGVmYXVsdCIsImZsb3djaGFydCI6eyJjdXJ2ZSI6ImJhc2lzIn19LCJ1cGRhdGVFZGl0b3IiOmZhbHNlfQ)\n",
    "\n",
    "These network diagrams are confusing because the definition of each node is not consistency.  So let's define our layers mathematically:\n",
    "\n",
    "1. input values: $\\mathbf{x}$\n",
    "2. hidden layer: $\\mathbf{\\hat{y}} = \\mathbf{x} \\cdot \\mathbf{v} $\n",
    "  - $\\hat{y}_{11} = x_{11} v_{11} + x_{12} v_{21} $\n",
    "  - $\\hat{y}_{21} = x_{21} v_{11} + x_{22} v_{21} $\n",
    "  - ...\n",
    "  - $\\hat{y}_{42} = x_{41} v_{12} + x_{42} v_{22} $\n",
    "  - $\\hat{y}_{43} = x_{41} v_{13} + x_{42} v_{23} $\n",
    "3. activation of hidden layer: $ \\mathbf{g}(\\mathbf{\\hat{y}}) = \\frac{1}{1+e^{-\\mathbf{\\hat{y}}}} $\n",
    "4. output layer: $\\mathbf{y} = \\mathbf{g} \\cdot \\mathbf{w}$\n",
    "  - $y_{11} = g_{11} w_{11} + g_{12} w_{21} + g_{13} w_{31} $\n",
    "  - ...\n",
    "  - $y_{41} = g_{41} w_{11} + g_{42} w_{21} + g_{43} w_{31} $\n",
    "5. activation of output layer: $ \\mathbf{f}(\\mathbf{y}) = \\frac{1}{1+e^{-y}} $\n",
    "\n",
    "So our neural network with its hidden layer is really expressed as\n",
    "\n",
    "$ \\mathbf{f}(\\mathbf{y}(\\mathbf{g}(\\mathbf{\\hat{y}}(\\mathbf{x})))) $\n",
    "\n",
    "We choose mean-squared error as our loss function:\n",
    "\n",
    "$ E = \\frac{1}{N} \\displaystyle \\sum^{N} (f - f_0)^2 $\n",
    "\n",
    "which has some subtlety since $f$ is really a matrix $\\mathbf{f}$, and therefore our loss function $E$ is really a matrix $\\mathbf{E}$ whose elements $E_{ij}$ are defined as:\n",
    "\n",
    "$ E_{ij} = \\displaystyle \\frac{(f_{ij} - f_{0,ij})^2}{N} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "Gradient descent in this multilayer case is very similar to gradient descent in the single-layer perceptron case.  A couple of key points:\n",
    "\n",
    "1. Instead of the output layer taking its input from the input observations, it takes its input from the output of the hidden layer.  The hidden layer takes its input from the input observations\n",
    "2. Each layer has its own set of weights, so gradient descent is applied to each layer in sequence.\n",
    "3. The gradient is calculated starting at the output layer, and the chain rule is then applied to connect the gradients of the two layers.\n",
    "\n",
    "The last point is probably why the process of updating weights throughout a multilayer network is called _backpropagation_.  You have to start at the output and calculate gradients in the reverse direction of the network.\n",
    "\n",
    "#### Output layer gradient\n",
    "\n",
    "Calculating the gradient of the error (loss function) with respect to the weights begins at the output layer:\n",
    "\n",
    "$ \\displaystyle \\frac{\\partial E}{\\partial w} =\n",
    "\\frac{\\partial E}{\\partial f} \\cdot\n",
    "\\frac{\\partial f}{\\partial y} \\cdot\n",
    "\\frac{\\partial y}{\\partial w}\n",
    "$\n",
    "\n",
    "where\n",
    "\n",
    "- $ \\frac{\\partial E}{\\partial f} = \\frac{2}{N} ( f - f_0 ) $\n",
    "- $ \\frac{\\partial f}{\\partial y} = f (1 - f) $\n",
    "- $ \\frac{\\partial y}{\\partial w} = g$ (in the single-layer case, $ \\frac{\\partial y}{\\partial w} = x $)\n",
    "\n",
    "This is the same as gradient descent in the single-layer perceptron case _except_ that the inputs from this layer come from the hidden layer's output, $g(\\hat{y})$, instead of the observations $x$.\n",
    "\n",
    "#### Hidden layer (\"phase 2\")\n",
    "\n",
    "Calculating the gradient of the hidden layer's weights is the same as the output layer, except the inputs and outputs from this layer are different.  Again, we want to calculate the gradient of the error $E$ with respect to this layer's weights $v$:\n",
    "\n",
    "$ \\displaystyle \\frac{\\partial E}{\\partial v} = \n",
    "\\frac{\\partial E}{\\partial g}\n",
    "\\frac{\\partial g}{\\partial \\hat{y}}\n",
    "\\frac{\\partial \\hat{y}}{\\partial v} $\n",
    "\n",
    "where\n",
    "\n",
    "- $ \\frac{\\partial E}{\\partial g} =\n",
    "           \\frac{\\partial E}{\\partial f} \\cdot\n",
    "           \\frac{\\partial f}{\\partial y} \\cdot\n",
    "           \\frac{\\partial y}{\\partial g} $\n",
    "    - $ \\frac{\\partial E}{\\partial g} =\n",
    "        \\frac{\\partial E}{\\partial y} \\cdot\n",
    "        \\frac{\\partial y}{\\partial g} $\n",
    "    - $ \\frac{\\partial E}{\\partial y} =\n",
    "        \\frac{\\partial E}{\\partial f} \\cdot\n",
    "        \\frac{\\partial f}{\\partial y}$\n",
    "- $ \\frac{\\partial g}{\\partial \\hat{y}} = g (1 - g)$\n",
    "- $ \\frac{\\partial \\hat{y}}{\\partial v} = x$\n",
    "\n",
    "We rely on chain-rule expanding $\\frac{\\partial E}{\\partial g}$ so we can solve for it in terms of the output layer's partial derivatives.  This is where the two layers are connected and how the gradient we calculated for the output layer help us calculate the gradient for this hidden layer.\n",
    "\n",
    "The only other differences is that this hidden layer takes its input from the observations provided at the beginning of training, $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error at step     0: 1.960156e-01\n",
      "error at step  1000: 1.651662e-01\n",
      "error at step  2000: 1.593565e-01\n",
      "error at step  3000: 1.550351e-01\n",
      "error at step  4000: 1.511067e-01\n",
      "error at step  5000: 1.465351e-01\n",
      "error at step  6000: 1.392272e-01\n",
      "error at step  7000: 1.234844e-01\n",
      "error at step  8000: 9.350530e-02\n",
      "error at step  9000: 6.205274e-02\n",
      "Final weights:\n",
      "  hidden: [ 1.66707072  2.43454525 -1.50667653  1.66659174  2.29166158 -1.50681196]\n",
      "  output: [ 0.70570663  2.04736287 -4.12867523]\n",
      "10000 iterations took 2.7 seconds\n"
     ]
    }
   ],
   "source": [
    "x = inputs\n",
    "    \n",
    "t0 = time.time()\n",
    "for i in range(NUM_ITERATIONS):\n",
    "\n",
    "    w = weights_output\n",
    "    v = weights_hidden\n",
    "    yhat = linear(x, v, bias)\n",
    "    g = sigmoid(yhat)\n",
    "    y = linear(g, w, bias)\n",
    "    f = sigmoid(y)\n",
    "    \n",
    "    error = numpy.power(f - ground_truth, 2).mean()\n",
    "    if i % (NUM_ITERATIONS / 10) == 0:\n",
    "        print(\"error at step {:5d}: {:10.6e}\".format(i, error))\n",
    "    \n",
    "    # calculate out partial derivatives for the output layer's weights\n",
    "    dE_df = 2.0 * (f - ground_truth) / f.size\n",
    "    df_dy = f * (1.0 - f)\n",
    "    dy_dw = g\n",
    "    dE_dy = dE_df * df_dy\n",
    "    dE_dw = numpy.dot(dy_dw.T, dE_dy)\n",
    "    \n",
    "    # calculate partial derivatives with respect to hidden layer weights\n",
    "    dy_dg = w\n",
    "    dE_dg = numpy.dot(dE_dy, dy_dg.T)\n",
    "    dg_dyhat = g * (1.0 - g)\n",
    "    dyhat_dv = x\n",
    "    dE_dyhat = dE_dg * dg_dyhat\n",
    "    dE_dv = numpy.dot(dyhat_dv.T, dE_dyhat)\n",
    "    \n",
    "    # update weights and biases - the error is the sum of error over each input\n",
    "    weights_hidden -= LEARNING_RATE * dE_dv\n",
    "    weights_output -= LEARNING_RATE * dE_dw\n",
    "#   bias -= LEARNING_RATE * dE_dy.sum()\n",
    "\n",
    "print(\"Final weights:\")\n",
    "print(\"  hidden: {}\".format(weights_hidden.flatten()))\n",
    "print(\"  output: {}\".format(weights_output.flatten()))\n",
    "#print(\"Final bias:    {}\".format(bias))\n",
    "print(\"{:d} iterations took {:.1f} seconds\".format(NUM_ITERATIONS, time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted vs. actual outputs:\n",
      "             input1  input2  output  prediction     error\n",
      "observation                                              \n",
      "1                 0       0       0    0.334522 -0.334522\n",
      "2                 0       1       1    0.846053  0.153947\n",
      "3                 1       0       1    0.849022  0.150978\n",
      "4                 1       1       1    0.925358  0.074642\n"
     ]
    }
   ],
   "source": [
    "# traverse the hidden layer\n",
    "predicted_output = linear(inputs, weights_hidden, bias)\n",
    "predicted_output = sigmoid(predicted_output)\n",
    "\n",
    "# traverse the output layer\n",
    "predicted_output = linear(predicted_output, weights_output, bias)\n",
    "predicted_output = sigmoid(predicted_output)\n",
    "\n",
    "# convert output to dataframe\n",
    "predicted_output = pandas.DataFrame(\n",
    "    predicted_output,\n",
    "    columns=[\"prediction\"],\n",
    "    index=dataset.index)\n",
    "\n",
    "output = pandas.concat(\n",
    "    (dataset, predicted_output),\n",
    "    axis=1)\n",
    "output['error'] = output['output'] - output['prediction']\n",
    "print(\"Predicted vs. actual outputs:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(inputs.shape[1], NUM_NEURONS_HIDDEN, bias=False),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(NUM_NEURONS_HIDDEN, 1, bias=False),\n",
    "    torch.nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hidden layer weights:\n",
      "Parameter containing:\n",
      "tensor([[4.1702e-01, 3.0233e-01],\n",
      "        [7.2032e-01, 1.4676e-01],\n",
      "        [1.1437e-04, 9.2339e-02]], requires_grad=True)\n",
      "\n",
      "Starting output layer weights:\n",
      "Parameter containing:\n",
      "tensor([[0.1863, 0.3456, 0.3968]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(seed=1) # use same initial seed as before\n",
    "\n",
    "with torch.no_grad():\n",
    "    model[0].weight = torch.nn.Parameter(torch.from_numpy(numpy.random.rand(inputs.shape[1], NUM_NEURONS_HIDDEN).astype(numpy.float32).T))\n",
    "    # model[0].bias = torch.nn.Parameter(torch.from_numpy(numpy.random.rand(1, 1)))\n",
    "    model[2].weight = torch.nn.Parameter(torch.from_numpy(numpy.random.rand(NUM_NEURONS_HIDDEN, 1).astype(numpy.float32).T))\n",
    "print(\"Starting hidden layer weights:\\n{}\\n\".format(model[0].weight))\n",
    "print(\"Starting output layer weights:\\n{}\".format(model[2].weight))\n",
    "# print(\"Starting bias: {}\".format(model[0].bias.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error at step     0: 1.960156e-01\n",
      "error at step  1000: 1.651661e-01\n",
      "error at step  2000: 1.593565e-01\n",
      "error at step  3000: 1.550351e-01\n",
      "error at step  4000: 1.511067e-01\n",
      "error at step  5000: 1.465351e-01\n",
      "error at step  6000: 1.392272e-01\n",
      "error at step  7000: 1.234844e-01\n",
      "error at step  8000: 9.350532e-02\n",
      "error at step  9000: 6.205267e-02\n",
      "Final weights:\n",
      "  hidden: [ 1.6670684  1.6665915  2.4345472  2.2916615 -1.5066769 -1.5068121]\n",
      "  output: [ 0.70570666  2.0473638  -4.128679  ]\n",
      "10000 iterations took 10.4 seconds\n"
     ]
    }
   ],
   "source": [
    "inputs_tensor = torch.from_numpy(inputs)\n",
    "truth_tensor = torch.from_numpy(ground_truth.reshape(-1, 1))\n",
    "\n",
    "loss = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "model.train()\n",
    "t0 = time.time()\n",
    "for i in range(NUM_ITERATIONS):\n",
    "    f = model(inputs_tensor)\n",
    "\n",
    "    error = loss(f, truth_tensor)\n",
    "    if i % (NUM_ITERATIONS / 10) == 0:\n",
    "        print(\"error at step {:5d}: {:10.6e}\".format(i, error))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    error.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Final weights:\")\n",
    "print(\"  hidden: {}\".format(next(model[0].parameters()).detach().numpy().flatten()))\n",
    "print(\"  output: {}\".format(next(model[2].parameters()).detach().numpy().flatten()))\n",
    "\n",
    "print(\"{:d} iterations took {:.1f} seconds\".format(NUM_ITERATIONS, time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted vs. actual outputs:\n",
      "             input1  input2  output  prediction     error\n",
      "observation                                              \n",
      "1                 0       0       0    0.334522 -0.334522\n",
      "2                 0       1       1    0.846053  0.153947\n",
      "3                 1       0       1    0.849022  0.150978\n",
      "4                 1       1       1    0.925358  0.074642\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "predicted_output = model(inputs_tensor).detach().numpy()\n",
    "predicted_output = pandas.DataFrame(\n",
    "    predicted_output,\n",
    "    columns=[\"prediction\"],\n",
    "    index=dataset.index)\n",
    "\n",
    "output = pandas.concat(\n",
    "    (dataset, predicted_output),\n",
    "    axis=1)\n",
    "output['error'] = output['output'] - output['prediction']\n",
    "print(\"Predicted vs. actual outputs:\")\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
